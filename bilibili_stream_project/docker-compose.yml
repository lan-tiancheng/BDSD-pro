services:
  zookeeper:
    image: zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_PORT: 2181
    restart: unless-stopped

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://:9092,PLAINTEXT_INTERNAL://:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: "bilibili_videos:1:1"
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      # 设置消息保留 24 小时
      KAFKA_LOG_RETENTION_MS: 86400000
    depends_on:
      - zookeeper
    restart: unless-stopped

  spark-master:
    image: bitnami/spark:3.2.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - TZ=Asia/Shanghai
    ports:
      - "7077:7077"
      - "8082:8080"
    volumes:
      - ./work:/opt/spark/work
    command: ["/opt/bitnami/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master", "--port", "7077", "--webui-port", "8080"]
    restart: unless-stopped

  spark-worker:
    image: bitnami/spark:3.2.3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - TZ=Asia/Shanghai
    ports:
      - "8081:8081"
    volumes:
      - ./work:/opt/spark/work
    depends_on:
      - spark-master
    command: ["/opt/bitnami/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    restart: unless-stopped

  fetcher:
    build:
      context: .
      dockerfile: Dockerfile.fetch
    container_name: fetcher
    environment:
      TZ: Asia/Shanghai
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      TOPIC: bilibili_videos
      FETCH_MODE: hot
      HOT_DAYS: "3"
      FETCH_INTERVAL: "300"        # 每5分钟一轮
      LIMIT: "60"
      DEDUP: "false"
      ENABLE_DETAIL_STAT: "false"   # 关闭 archive/stat 详情接口
      ENABLE_VIEW_DETAIL: "false"   # 关闭 view 详情接口
      DETAIL_CALL_LIMIT: "0"
      DETAIL_CACHE_TTL: "600"
      DEBUG_FETCH: "true"
      DROP_ZERO_VIDEOS: "false"
      PARSE_CHINESE_NUM: "true"
    depends_on:
      - kafka
    restart: unless-stopped

  spark-consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: spark-consumer
    environment:
      TZ: Asia/Shanghai
      KAFKA_BOOTSTRAP: kafka:9092
      TOPIC: bilibili_videos
      WEIGHT_VIEW: "0.05"
      WEIGHT_LIKE: "1.0"
      WEIGHT_COIN: "1.2"
      WEIGHT_FAVORITE: "1.0"
      WEIGHT_DANMAKU: "1.1"
      WEIGHT_REPLY: "1.3"
      WEIGHT_SHARE: "1.4"
      DECAY_LAMBDA: "0.05"
      SIMPLE_TEST_MODE: "false"     # 使用24小时窗口
      DEBUG_METRICS: "true"
      SHOW_SAMPLE_ROWS: "0"
      NO_DECAY: "false"
      DISABLE_CODEGEN: "true"
      STARTING_OFFSETS: "earliest"    # 从最早位置开始消费
      ENABLE_VIDEO_TOPN: "true"
      TOPN: "3"
      CONSUMER_TRIGGER_SECONDS: "60"
      ROUND_DRIVEN: "false"
      SKIP_ZERO_WINDOWS: "true"
      MIN_NONZERO_CATEGORIES: "2"
      MIN_TOTAL_RAW_SUM: "10"
    volumes:
      - consumer_ivy_cache:/opt/spark/ivy
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --conf spark.jars.ivy=/opt/spark/ivy
      --conf spark.sql.shuffle.partitions=4
      --conf spark.jars.repositories=https://maven.aliyun.com/repository/central,https://maven.aliyun.com/repository/public
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.3
      /opt/spark/app/realtime_consumer.py
    depends_on:
      - kafka
      - spark-master
    restart: unless-stopped

volumes:
  consumer_ivy_cache:
    driver: local
